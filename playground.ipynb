{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_agents.planner_agent import PlannerAgent\n",
    "\n",
    "planner_agent = PlannerAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98298f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await planner_agent.execute(\"Make a SWOT analysis for apple Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_agents.base import BaseAgent\n",
    "import os \n",
    "agent = BaseAgent(\"test bot\", \"\", os.getenv(\"\"))\n",
    "await agent.execute(\"Make a SWOT analysis for apple Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI, AsyncAzureOpenAI\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f471619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, OpenAIChatCompletionsModel\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    model=OpenAIChatCompletionsModel(\n",
    "        model=os.getenv(\"GPT4O_MINI_DEPLOYMENT\"),\n",
    "        openai_client=client,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"Make a SWOT analysis for apple Inc.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f914f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.raw_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agents.swot_agent import SWOTAgent\n",
    "\n",
    "planner_agent = SWOTAgent()\n",
    "resutl = await planner_agent.execute(\"Make a SWOT analysis for apple Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in resutl.final_output:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agents.base_agent import BaseAgent\n",
    "\n",
    "base_agent = BaseAgent(\n",
    "    name=\"Chatbot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await base_agent.execute(\"Wie alt wurde Albert Einstein am 14. MÃ¤rz 1879?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "from manager import SWOTAnalysisManager\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# print(os.getenv(\"O3_MINI_DEPLOYMENT\"))\n",
    "\n",
    "swot_manager = SWOTAnalysisManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await swot_manager.run(\"Make a very detailed SWOT analysis for SAP and use data from 2025.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6b738",
   "metadata": {},
   "source": [
    "# Baue den SWOTAnalysisManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a8374",
   "metadata": {},
   "source": [
    "## Aufbau\n",
    "1. Planner\n",
    "2. Searcher\n",
    "3. SWOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77920c3",
   "metadata": {},
   "source": [
    "### 1. Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e482c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "query = \"Make a very detailed SWOT analysis for SAP and use data from 2025.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e278ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agents.planner_agent import PlannerAgent, WebSearchPlan, WebSearchItem\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "planner_agent = PlannerAgent()\n",
    "planner_result = planner_agent.execute_streamed(f\"Query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plan in planner_result.final_output_as(WebSearchPlan).searches:\n",
    "    print(f\"{plan.query} ----> {plan.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933d242",
   "metadata": {},
   "source": [
    "### 2. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56221d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agents.search_agent import SearchAgent\n",
    "\n",
    "search_agent = SearchAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search(item: WebSearchItem):\n",
    "    input_data = f\"Search term: {item.query}\\nReason: {item.reason}\\n\"\n",
    "    result = await search_agent.execute(input_data)\n",
    "    return result.final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents import custom_span\n",
    "\n",
    "tasks = [asyncio.create_task(search(item)) for item in planner_result.final_output_as(WebSearchPlan).searches]\n",
    "\n",
    "with custom_span(\"Search for all items\"):\n",
    "    # await asyncio.gather(*tasks)\n",
    "    search_resutls = []\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        result = await task\n",
    "        search_resutls.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_resutls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60191573",
   "metadata": {},
   "source": [
    "### 3. SWOT Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5479f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_agents.swot_agent import SWOTAgent\n",
    "\n",
    "swot_agent = SWOTAgent()\n",
    "\n",
    "input_data = f\"Original query: {query}\\nSummarized search results: {search_resutls}\"\n",
    "\n",
    "swot_result = await swot_agent.execute(input_data, max_turns=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "swot_result.final_output.swot_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86a263",
   "metadata": {},
   "source": [
    "### 4. Test New Manager Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878717c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from manager import SWOTAnalysisManager\n",
    "\n",
    "query = \"Make a very detailed SWOT analysis for SAP and use data from 2025.\"\n",
    "\n",
    "swot_manager = SWOTAnalysisManager()\n",
    "swot_result = await swot_manager.execute_streamed(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in swot_result.stream_events():\n",
    "    if isinstance(event, ResponseTextDeltaEvent):\n",
    "        print(event.delta.content, end=\"\")\n",
    "    else:\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2f31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2c28544",
   "metadata": {},
   "source": [
    "# New Idea\n",
    "## Aufbau\n",
    "1. Planner ohne Stream\n",
    "2. Search ohne Stream\n",
    "3. SWOT mit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1987c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from manager import SWOTAnalysisManager\n",
    "\n",
    "agent = SWOTAnalysisManager()\n",
    "\n",
    "query = \"Make a SWOT analysis for Deutsche Telekom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_result = await agent.planner_agent.execute(f\"Query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff94189",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "planner_answer = \"\"\n",
    "for item in planner_result.final_output.searches:\n",
    "    planner_answer += f\"{n}. Search Query: {item.query} <--> Reason {item.reason}\\n\"\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82ed03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99b51672",
   "metadata": {},
   "source": [
    "# 10k-Filings Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1735dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the necessary functions from edgartools\n",
    "from edgar import *\n",
    "\n",
    "# 2. Tell the SEC who you are\n",
    "set_identity(\"mike@indigo.com\")\n",
    "\n",
    "# 3. Start using the library\n",
    "filings = get_filings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor of SAP\n",
    "competitors = {\n",
    "    \"Competitors_by_Sector\": {\n",
    "        \"Apple Inc.\": \"AAPL\",\n",
    "        \"Microsoft Corporation\": \"MSFT\",\n",
    "        \"NVIDIA Corporation\": \"NVDA\",\n",
    "        \"Broadcom Inc.\": \"AVGO\",\n",
    "        \"Oracle Corporation\": \"ORCL\",\n",
    "        \"Salesforce, Inc.\": \"CRM\",\n",
    "        \"Cisco Systems, Inc.\": \"CSCO\",\n",
    "        \"International Business Machines Corporation\": \"IBM\",\n",
    "        \"Palantir Technologies Inc.\": \"PLTR\",\n",
    "        \"Accenture plc\": \"ACN\"\n",
    "    },\n",
    "    \"Competitors_by_Industry\": {\n",
    "        \"Salesforce, Inc.\": \"CRM\",\n",
    "        \"Intuit Inc.\": \"INTU\",\n",
    "        \"ServiceNow, Inc.\": \"NOW\",\n",
    "        \"Adobe Inc.\": \"ADBE\",\n",
    "        \"Uber Technologies, Inc.\": \"UBER\",\n",
    "        \"Automatic Data Processing, Inc.\": \"ADP\",\n",
    "        \"AppLovin Corporation\": \"APP\",\n",
    "        \"Cadence Design Systems, Inc.\": \"CDNS\",\n",
    "        \"MicroStrategy Incorporated\": \"MSTR\",\n",
    "        \"Workday, Inc.\": \"WDAY\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c31363",
   "metadata": {},
   "source": [
    "### Retrieve latest 3 10k forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the necessary functions from edgartools\n",
    "from edgar import set_identity, get_filings, Company\n",
    "\n",
    "def retrieve_tenk_forms(company_ticker: str, top_k: int, path: str):\n",
    "    set_identity(\"mike@indigo.com\")\n",
    "    # filings = get_filings()\n",
    "    company = Company(company_ticker)\n",
    "    tenk_filings = company.get_filings(form=\"10-K\")\n",
    "\n",
    "    i = 1\n",
    "    for filing in tenk_filings:\n",
    "        if i <= top_k:\n",
    "            filing_name = filing.attachments[1].document.split(sep=\".\")[0]\n",
    "            filing.attachments[1].download(path=f\"{path}/{filing_name}.html\")\n",
    "        i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "for company_name, ticker in competitors[\"Competitors_by_Sector\"].items():\n",
    "    print(f\"Safe top {top_k} 10k forms for {company_name}\")\n",
    "    retrieve_tenk_forms(ticker, top_k, \"./10k-data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "for company_name, ticker in competitors[\"Competitors_by_Industry\"].items():\n",
    "    print(f\"Safe top {top_k} 10k forms for {company_name}\")\n",
    "    retrieve_tenk_forms(ticker, top_k, \"./10k-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_tenk_forms('SAP', 4, \"./10k-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = Company(\"CRM\") # Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d105096",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenk_filings = company.get_filings(form=\"10-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49161765",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for filing in tenk_filings:\n",
    "    if i <= 3:\n",
    "        filing_name = filing.attachments[1].document.split(sep=\".\")[0]\n",
    "        filing.attachments[1].download(path=f\"./10k-data/{filing_name}.html\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "filings.attachments[1].download(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filing = filings.attachments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6cb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253596f8",
   "metadata": {},
   "source": [
    "# 10k Filings Chunking, Embedding and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede82fdf",
   "metadata": {},
   "source": [
    "### SAP competitors dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a15e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor of SAP\n",
    "competitors = {\n",
    "    \"Competitors_by_Sector\": {\n",
    "        \"Apple Inc.\": \"AAPL\",\n",
    "        \"Microsoft Corporation\": \"MSFT\",\n",
    "        \"NVIDIA Corporation\": \"NVDA\",\n",
    "        \"Broadcom Inc.\": \"AVGO\",\n",
    "        \"Oracle Corporation\": \"ORCL\",\n",
    "        \"Salesforce, Inc.\": \"CRM\",\n",
    "        \"Cisco Systems, Inc.\": \"CSCO\",\n",
    "        \"International Business Machines Corporation\": \"IBM\",\n",
    "        \"Palantir Technologies Inc.\": \"PLTR\",\n",
    "        \"Accenture plc\": \"ACN\"\n",
    "    },\n",
    "    \"Competitors_by_Industry\": {\n",
    "        \"Salesforce, Inc.\": \"CRM\",\n",
    "        \"Intuit Inc.\": \"INTU\",\n",
    "        \"ServiceNow, Inc.\": \"NOW\",\n",
    "        \"Adobe Inc.\": \"ADBE\",\n",
    "        \"Uber Technologies, Inc.\": \"UBER\",\n",
    "        \"Automatic Data Processing, Inc.\": \"ADP\",\n",
    "        \"AppLovin Corporation\": \"APP\",\n",
    "        \"Cadence Design Systems, Inc.\": \"CDNS\",\n",
    "        \"MicroStrategy Incorporated\": \"MSTR\",\n",
    "        \"Workday, Inc.\": \"WDAY\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0a091",
   "metadata": {},
   "source": [
    "### Parse HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1982752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def split_into_pages(html_content):\n",
    "    \"\"\"\n",
    "    Splits the HTML content into pages using BeautifulSoup.\n",
    "    \n",
    "    Looks for an <hr> tag with a style attribute that includes \n",
    "    'page-break-after:always' and uses these as page delimiters.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    pages = []\n",
    "    current_page_elements = []\n",
    "\n",
    "    # Try to get the body if available; else use the entire soup\n",
    "    container = soup.body if soup.body else soup\n",
    "\n",
    "    # Iterate over all top-level elements in the container\n",
    "    for element in container.contents:\n",
    "        # Check if the element is a Tag and whether it is a page break\n",
    "        if isinstance(element, Tag) and element.name == 'hr':\n",
    "            style = element.get('style', '')\n",
    "            if 'page-break-after:always' in style:\n",
    "                # Page break encountered; add the current page to the pages list\n",
    "                page_html = ''.join(str(item) for item in current_page_elements).strip()\n",
    "                if page_html:\n",
    "                    pages.append(page_html)\n",
    "                # Reset the current page elements for the next page\n",
    "                current_page_elements = []\n",
    "                continue\n",
    "        \n",
    "        # Otherwise, add the element to the current page elements\n",
    "        current_page_elements.append(element)\n",
    "\n",
    "    # Append the last page (if any content remains)\n",
    "    page_html = ''.join(str(item) for item in current_page_elements).strip()\n",
    "    if page_html:\n",
    "        pages.append(page_html)\n",
    "    return pages\n",
    "\n",
    "def process_pages(pages: list):\n",
    "    processed_pages = []\n",
    "    for page in pages:\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        if \"Table of Contents\" == text[:len(\"Table of Contents\")]:\n",
    "            processed_pages.append(text[len(\"Table of Contents\"):])\n",
    "        else:\n",
    "            processed_pages.append(text)\n",
    "    return processed_pages\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads an HTML file, splits its content into pages using BeautifulSoup,\n",
    "    and returns a list containing the HTML of each page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "        pages = split_into_pages(html_content)\n",
    "        processed_pages = process_pages(pages)\n",
    "        return pages, processed_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af756972",
   "metadata": {},
   "source": [
    "### Process HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_aapl_file.py\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from ingest.chunking import chunk_text\n",
    "from ingest.embedding import generate_embedding\n",
    "from vectorstore.azure_search_client import upload_documents_to_index\n",
    "\n",
    "def process_html_file(file_path: str, company_name: str, max_chunk_size: int = 20000):\n",
    "    \"\"\"\n",
    "    Reads an HTML file, extracts text using BeautifulSoup,\n",
    "    splits it into smaller chunks, computes embeddings for each chunk,\n",
    "    and returns a list of documents ready to be uploaded.\n",
    "    \"\"\"\n",
    "    # # Read the HTML file from disk\n",
    "    # with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    #     html_content = f.read()\n",
    "    \n",
    "    # # Use BeautifulSoup to extract plain text\n",
    "    # soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    # text = soup.get_text(separator=\"\\n\")\n",
    "    \n",
    "    # # Split the extracted text into chunks (default: chunk_size=1000, overlap=100)\n",
    "    # chunks = chunk_text(text)\n",
    "    _, chunks = process_file(file_path)\n",
    "    file_name = file_path.split(\"/\")[-1][:-5]\n",
    "    print(f\"Processing file {file_name}\")\n",
    "    year = int(file_name[-8:-4])\n",
    "    documents = []\n",
    "    token_text_ratios = []\n",
    "    # Process each chunk: compute embedding and create a document structure\n",
    "    for index, chunk in enumerate(chunks, start=1):\n",
    "        # Compute the embedding for this text chunk\n",
    "        print(f\"-- Processing chunk {index}/{len(chunks)} for {file_name}\")\n",
    "        if len(chunk) >= max_chunk_size:\n",
    "            for i in range(0, len(chunk), max_chunk_size):\n",
    "                sub_chunk = chunk[i:i+max_chunk_size]\n",
    "                embedding, token_text_ratio = generate_embedding(sub_chunk)\n",
    "                # Create a unique identifier for the chunk\n",
    "                doc_id = f\"{file_name}_{index}_{i//max_chunk_size}\"\n",
    "                document = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"data\": sub_chunk,\n",
    "                    \"dataVector\": embedding,\n",
    "                    # Additional metadata can be added below as needed:\n",
    "                    \"metadata\": f\"10k filing page {index} for {company_name}\",\n",
    "                    \"year\": year,\n",
    "                    \"companyname\": company_name,\n",
    "                    \"tenkpage\": index\n",
    "                }\n",
    "                documents.append(document)\n",
    "                token_text_ratios.append(token_text_ratio)\n",
    "        else:\n",
    "            embedding, token_text_ratio = generate_embedding(chunk)\n",
    "            # Create a unique identifier for the chunk\n",
    "            doc_id = f\"{file_name}_{index}\"\n",
    "            document = {\n",
    "                \"id\": doc_id,\n",
    "                \"data\": chunk,\n",
    "                \"dataVector\": embedding,\n",
    "                # Additional metadata can be added below as needed:\n",
    "                \"metadata\": f\"10k filing page {index} for {company_name}\",\n",
    "                \"year\": year,\n",
    "                \"companyname\": company_name,\n",
    "                \"tenkpage\": index\n",
    "            }\n",
    "            documents.append(document)\n",
    "            token_text_ratios.append(token_text_ratio)\n",
    "    \n",
    "    return documents, token_text_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder containing the HTML files\n",
    "folder_path = \"./10k-data\"\n",
    "\n",
    "def find_html_files(folder_path: str, ticker: str):\n",
    "\n",
    "    # Iterate through all files in the folder and find those containing 'aapl'\n",
    "    files = [\n",
    "        file_name for file_name in os.listdir(folder_path)\n",
    "        if file_name.endswith(\".html\") and ticker in file_name.lower()\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file and create a list of entries\n",
    "with open(\"./processed_files.txt\", \"r\") as file:\n",
    "    processed_files_list = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Print the list to verify\n",
    "print(processed_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"./10k-data/avgo-20231029.html\"\n",
    "folder_path = \"./10k-data\"\n",
    "# file_name = file_path.split(\"/\")[-1][:-5]\n",
    "# process_html_file(\"./10k-data/wday-20230131.html\", \"Workday, Inc.\")\n",
    "\n",
    "file_token_text_ratios = {}\n",
    "for key, sap_competitors in competitors.items():\n",
    "    print(f\"Processing {key}\")\n",
    "    for competitor, ticker in sap_competitors.items():\n",
    "        files = find_html_files(folder_path, ticker.lower())\n",
    "        for file in files:\n",
    "            if file in processed_files_list:\n",
    "                print(f\"File {file} already processed. Skipping...\")\n",
    "                continue\n",
    "            file_path = folder_path + \"/\" + file\n",
    "            documents, token_text_rations = process_html_file(file_path, competitor, 20000)\n",
    "            file_token_text_ratios[file_path] = token_text_rations\n",
    "            print(f\"documents: {documents}\")\n",
    "            try:\n",
    "                upload_documents_to_index(documents=documents)\n",
    "                with open(\"./processed_files.txt\", \"a\") as processed_file:\n",
    "                    processed_file.write(file + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Pass file {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your local HTML file\n",
    "file_path = \"aapl-20240928.html\"\n",
    "\n",
    "# Process the HTML file into chunked documents with embeddings\n",
    "documents = process_html_file(file_path)\n",
    "print(f\"Processed {len(documents)} chunks from {file_path}.\")\n",
    "\n",
    "# Upload the processed documents to your Azure Cognitive Search index\n",
    "upload_documents_to_index(documents)\n",
    "print(\"Documents uploaded to Azure Search index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad801996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder containing the HTML files\n",
    "folder_path = \"./10k-data\"\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".html\"):  # Check if the file is an HTML file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Add your processing logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(file_name[-8:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0c696",
   "metadata": {},
   "source": [
    "# Read HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = \"./10k-data/crm-20250131.html\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Use BeautifulSoup to extract plain text\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "text = soup.get_text(separator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03214f5a",
   "metadata": {},
   "source": [
    "### Parse HTML by pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def split_into_pages(html_content):\n",
    "    \"\"\"\n",
    "    Splits the HTML content into pages using BeautifulSoup.\n",
    "    \n",
    "    Looks for an <hr> tag with a style attribute that includes \n",
    "    'page-break-after:always' and uses these as page delimiters.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    pages = []\n",
    "    current_page_elements = []\n",
    "\n",
    "    # Try to get the body if available; else use the entire soup\n",
    "    container = soup.body if soup.body else soup\n",
    "\n",
    "    # Iterate over all top-level elements in the container\n",
    "    for element in container.contents:\n",
    "        # Check if the element is a Tag and whether it is a page break\n",
    "        if isinstance(element, Tag) and element.name == 'hr':\n",
    "            style = element.get('style', '')\n",
    "            if 'page-break-after:always' in style:\n",
    "                # Page break encountered; add the current page to the pages list\n",
    "                page_html = ''.join(str(item) for item in current_page_elements).strip()\n",
    "                if page_html:\n",
    "                    pages.append(page_html)\n",
    "                # Reset the current page elements for the next page\n",
    "                current_page_elements = []\n",
    "                continue\n",
    "        \n",
    "        # Otherwise, add the element to the current page elements\n",
    "        current_page_elements.append(element)\n",
    "\n",
    "    # Append the last page (if any content remains)\n",
    "    page_html = ''.join(str(item) for item in current_page_elements).strip()\n",
    "    if page_html:\n",
    "        pages.append(page_html)\n",
    "    return pages\n",
    "\n",
    "def process_pages(pages: list):\n",
    "    processed_pages = []\n",
    "    for page in pages:\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        if \"Table of Contents\" == text[:len(\"Table of Contents\")]:\n",
    "            processed_pages.append(text[len(\"Table of Contents\"):])\n",
    "    return processed_pages\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads an HTML file, splits its content into pages using BeautifulSoup,\n",
    "    and returns a list containing the HTML of each page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "        pages = split_into_pages(html_content)\n",
    "        processed_pages = process_pages(pages)\n",
    "        return pages, processed_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3267aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./10k-data/crm-20250131.html\"\n",
    "# file_path = \"./10k-data/wday-20250131.html\"\n",
    "pages, process_pages = process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    text = soup.get_text()#[len(\"Table of Contents\"):]\n",
    "    # text[:len(\"Table of Contents\")]\n",
    "    if \"Table of Contents\" == text[:len(\"Table of Contents\")]:\n",
    "        print(text[len(\"Table of Contents\"):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71b0fb",
   "metadata": {},
   "source": [
    "### Parse PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import argparse\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "\n",
    "def parse_pdf(file_path, start_string=\"Integrated  Report  2024   \\nTo Our  \\nStakeholders  Consolidated Group \\nManagement Report  Consolidated Financial \\nStatements IFRS  Additional  \\nInformation  \\n \\n\"):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    Returns a string containing the concatenated text of all pages.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        pages = []\n",
    "        for page in reader.pages:\n",
    "            pages.append(page.extract_text().replace(start_string, \"\"))\n",
    "            # page_text = page.extract_text()\n",
    "            # if page_text:\n",
    "            #     text += page_text + \"\\n\"\n",
    "        return pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF {file_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingest.embedding import generate_embedding\n",
    "from vectorstore.azure_search_client import upload_documents_to_index\n",
    "\n",
    "def process_pdf_file(file_path: str, company_name: str, max_chunk_size: int = 20000):\n",
    "    \"\"\"\n",
    "    Reads an pdf file, extracts text,\n",
    "    splits it into smaller chunks, computes embeddings for each chunk,\n",
    "    and returns a list of documents ready to be uploaded.\n",
    "    \"\"\"\n",
    "    chunks = parse_pdf(file_path)\n",
    "    file_name = file_path.split(\"/\")[-1][:-4]\n",
    "    year = int(file_name[:4])\n",
    "    print(f\"Processing file {file_name}\")\n",
    "    documents = []\n",
    "    token_text_ratios = []\n",
    "    # Process each chunk: compute embedding and create a document structure\n",
    "    for index, chunk in enumerate(chunks, start=1):\n",
    "        # Compute the embedding for this text chunk\n",
    "        print(f\"-- Processing chunk {index}/{len(chunks)} for {file_name}\")\n",
    "        if len(chunk) >= max_chunk_size:\n",
    "            for i in range(0, len(chunk), max_chunk_size):\n",
    "                sub_chunk = chunk[i:i+max_chunk_size]\n",
    "                embedding, token_text_ratio = generate_embedding(sub_chunk)\n",
    "                # Create a unique identifier for the chunk\n",
    "                doc_id = f\"{file_name}_{index}_{i//max_chunk_size}\"\n",
    "                document = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"data\": sub_chunk,\n",
    "                    \"dataVector\": embedding,\n",
    "                    # Additional metadata can be added below as needed:\n",
    "                    \"metadata\": f\"10k filing page {index} for {company_name}\",\n",
    "                    \"year\": year,\n",
    "                    \"companyname\": company_name,\n",
    "                    \"tenkpage\": index\n",
    "                }\n",
    "                documents.append(document)\n",
    "                token_text_ratios.append(token_text_ratio)\n",
    "        else:\n",
    "            embedding, token_text_ratio = generate_embedding(chunk)\n",
    "            # Create a unique identifier for the chunk\n",
    "            doc_id = f\"{file_name}_{index}\"\n",
    "            document = {\n",
    "                \"id\": doc_id,\n",
    "                \"data\": chunk,\n",
    "                \"dataVector\": embedding,\n",
    "                # Additional metadata can be added below as needed:\n",
    "                \"metadata\": f\"10k filing page {index} for {company_name}\",\n",
    "                \"year\": year,\n",
    "                \"companyname\": company_name,\n",
    "                \"tenkpage\": index\n",
    "            }\n",
    "            documents.append(document)\n",
    "            token_text_ratios.append(token_text_ratio)\n",
    "    \n",
    "    return documents, token_text_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee493478",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./10k-data/2023-SAP-Integrated-Report.pdf\"\n",
    "\n",
    "documents, token_text_rations = process_pdf_file(file_path, 'SAP', 20000)\n",
    "print(f\"documents: {documents} and token_text_rations: {token_text_rations}\")\n",
    "upload_documents_to_index(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189333cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_token_text_ratio = np.mean(token_text_rations)\n",
    "print(f\"Mean token text ratio: {mean_token_text_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099537a",
   "metadata": {},
   "source": [
    "# RAG Bot for 10k Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c8b9c",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from custom_agents.config import settings\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=settings.azure_openai_api_version,\n",
    "    openai_api_key=settings.azure_openai_api_key,\n",
    "    azure_endpoint=settings.azure_openai_endpoint,\n",
    "    azure_deployment=settings.gpt4o_mini_deployment,\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(settings.azure_search_api_key)\n",
    "search_client = SearchClient(endpoint=settings.azure_search_service_endpoint,\n",
    "                      index_name=settings.azure_search_index_name,\n",
    "                      credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c23712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever  \n",
    "\n",
    "class AzureSearchRetriever(BaseRetriever):\n",
    "    # declare these as Pydantic fields:\n",
    "    search_client: SearchClient\n",
    "    k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        results = self.search_client.search(search_text=query, top=self.k)\n",
    "        docs: List[Document] = []\n",
    "        for r in results:\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=r[\"data\"],\n",
    "                    metadata={\"company\": r[\"companyname\"], \"year\": r[\"year\"], \"tenkpage\": r[\"tenkpage\"], \"metadata\": r[\"metadata\"]},\n",
    "                )\n",
    "            )\n",
    "        return docs\n",
    "\n",
    "\n",
    "retriever = AzureSearchRetriever(search_client=search_client, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e657bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate \n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a company analysis research planner. Given a request for SWOT analysis, \n",
    "you are given a search query to retrieve chunks of 10k filings, which are stored in a vector storage.\n",
    "Provide a summary of the search results and then answer the question based on the context.\n",
    "Include financial data if available in the context. \n",
    "Identify the company name and year from the context and state them in the answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",               # or \"map_reduce\", \"refine\", etc.\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Apple Total Assets 2023 Balance Sheet\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"â¶ï¸ Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nð Sources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.metadata['company']} ({doc.metadata['year']}, {doc.metadata['tenkpage']}): {doc.page_content[:500]}â¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e48d87",
   "metadata": {},
   "source": [
    "### Planner LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from custom_agents.config import settings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "planner_llm = AzureChatOpenAI(\n",
    "    openai_api_version=settings.azure_openai_api_version,\n",
    "    openai_api_key=settings.azure_openai_api_key,\n",
    "    azure_endpoint=settings.azure_openai_endpoint,\n",
    "    azure_deployment=settings.gpt4o_mini_deployment,\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "planner_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\"],\n",
    "    template=\"\"\"\n",
    "You are a company analysis research planner. Given a request for SWOT analysis, \n",
    "generate document search queries that help to gather comprehensive information on the company. \n",
    "You are generating search queries to search in 10k filings. \n",
    "Think about what financial information is relevant to the SWOT analysis and how it can be extracted from the 10k filings.\n",
    "Provide between 5 and 15 detailed and relevant search queries, each clearly justified. \n",
    "The queries should be specific to the company and the year of the 10k filings.\n",
    "The queries should ALWAYS be in the format WITHOUT any justification: ['search query 1', 'search query 2', ...]\n",
    "The queries should be relevant to the following user query:\n",
    "{user_query}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "planner = planner_prompt | planner_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = planner.invoke({\"user_query\": \"Make a SWOT analysis for Apple in 2025.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "search_queries = ast.literal_eval(result.content)\n",
    "\n",
    "for search_query in search_queries:\n",
    "    result = qa_chain.invoke({\"query\": search_query})\n",
    "    print(f\"Query: {result['query']} - Result: {result['result']}\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(f\"- {doc.metadata['company']} ({doc.metadata['year']}, {doc.metadata['tenkpage']}): {doc.page_content[:500]}â¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1239fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
